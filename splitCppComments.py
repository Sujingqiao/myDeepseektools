import os
import re
import argparse
from pathlib import Path
from typing import List, Tuple

def is_header_comment_block(comment_lines: List[str], file_path: str) -> bool:
    """
    判断一个注释块是否属于文件顶部的版权/文件信息注释。
    通过关键词和位置（在文件很靠前）判断。
    """
    # 常见的文件头部注释关键词
    header_keywords = [
        'copyright', '@file', '@brief', '@author', '@version', '@date',
        'license', 'spdx', 'generated', 'do not edit', 'autogenerated',
        'file:', 'created by', 'project:'
    ]
    
    # 将所有行合并成一个字符串，转为小写
    comment_text = ' '.join(line.strip().lower() for line in comment_lines)
    
    # 检查是否包含任何头部关键词
    if any(keyword in comment_text for keyword in header_keywords):
        return True
        
    # 进一步检查：是否是简单的文件名/描述
    simple_patterns = [
        r'\b\w+\.cpp\b', r'\b\w+\.h\b', r'\b\w+\.hpp\b',  # 文件名
        r'^\s*[-=]+\s*$',  # 分隔线
    ]
    for line in comment_lines:
        stripped = line.strip().lower()
        if any(re.search(pattern, stripped) for pattern in simple_patterns):
            return True
            
    return False

def extract_long_comments_from_file(file_path: Path, min_lines: int = 4) -> List[Tuple[List[str], int]]:
    """
    从单个 C++ 源文件中提取超过 min_lines 行的注释块。
    返回: [(comment_lines, start_line_number), ...]
    """
    comments = []
    current_comment = []
    start_line = 0
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
    except (UnicodeDecodeError, PermissionError, OSError) as e:
        print(f"无法读取文件 {file_path}: {e}")
        return []
    
    i = 0
    while i < len(lines):
        line = lines[i]
        
        # 匹配以 /// 或 // 开头的行（忽略前面的空格）
        if re.match(r'^\s*//[/!]', line):
            # 开始一个新的注释块
            if not current_comment:
                start_line = i + 1  # 行号从1开始
            
            # 清理行：去掉前面的空格和 /////
            cleaned_line = re.sub(r'^\s*//[/!]? ?', '', line).rstrip()
            current_comment.append(cleaned_line)
            
            i += 1
            # 继续读取后续的 // 行
            while i < len(lines) and re.match(r'^\s*//[/!]', lines[i]):
                cleaned_next = re.sub(r'^\s*//[/!]? ?', '', lines[i]).rstrip()
                current_comment.append(cleaned_next)
                i += 1
            
            # 检查当前注释块是否足够长
            if len(current_comment) >= min_lines:
                # 这里不立即判断是否为 header，因为我们需要先收集所有长注释
                comments.append((current_comment.copy(), start_line))
            
            # 重置
            current_comment.clear()
        else:
            i += 1
    
    return comments

def filter_out_header_comments(all_comments: List[Tuple[List[str], int, Path]], 
                              threshold_lines: int = 20) -> List[Tuple[List[str], int, Path]]:
    """
    过滤掉可能是文件头部的注释块。
    策略：如果注释块出现在文件的前 threshold_lines 行，并且被判定为 header，则过滤。
    """
    filtered = []
    for comment_lines, start_line, file_path in all_comments:
        if start_line <= threshold_lines:
            if is_header_comment_block(comment_lines, file_path):
                continue  # 跳过这个头部注释
        filtered.append((comment_lines, start_line, file_path))
    
    return filtered

def main():
    parser = argparse.ArgumentParser(description='提取 C++ 源码中超过指定行数的长注释块。')
    parser.add_argument('root_dir', type=str, help='要遍历的根目录路径')
    parser.add_argument('-o', '--output', type=str, default='long_comments.txt', 
                        help='输出文件路径 (默认: long_comments.txt)')
    parser.add_argument('-n', '--min-lines', type=int, default=4, 
                        help='注释块最少行数 (默认: 4)')
    parser.add_argument('--include-h', action='store_true', 
                        help='是否包含 .h/.hpp 头文件 (默认只处理 .cpp)')
    
    args = parser.parse_args()
    
    root_dir = Path(args.root_dir)
    if not root_dir.exists() or not root_dir.is_dir():
        print(f"错误: 目录 '{args.root_dir}' 不存在或不是目录。")
        return
    
    # 定义源文件后缀
    extensions = ['.cpp', '.cc', '.cxx']
    if args.include_h:
        extensions.extend(['.h', '.hpp', '.hxx'])
    
    print(f"正在扫描目录: {root_dir}")
    print(f"查找后缀为 {extensions} 的文件中，长度 >= {args.min_lines} 行的注释...")
    
    all_long_comments = []  # 存储 (comment_lines, start_line, file_path)
    
    # 遍历所有文件
    for ext in extensions:
        for file_path in root_dir.rglob(f'*{ext}'):
            print(f"  处理文件: {file_path}")
            comments = extract_long_comments_from_file(file_path, args.min_lines)
            for comment_lines, start_line in comments:
                all_long_comments.append((comment_lines, start_line, file_path))
    
    # 过滤掉文件头部的注释
    final_comments = filter_out_header_comments(all_long_comments)
    
    # 写入输出文件
    try:
        with open(args.output, 'w', encoding='utf-8') as out_f:
            out_f.write(f"=== 长注释提取报告 ===\n")
            out_f.write(f"根目录: {root_dir.resolve()}\n")
            out_f.write(f"最小行数: {args.min_lines}\n")
            out_f.write(f"包含头文件: {'是' if args.include_h else '否'}\n")
            out_f.write(f"提取时间: {os.times().elapsed}\n")
            out_f.write(f"找到的长注释块数量: {len(final_comments)}\n")
            out_f.write("\n" + "="*80 + "\n\n")
            
            for i, (comment_lines, start_line, file_path) in enumerate(final_comments, 1):
                rel_path = file_path.relative_to(root_dir) if file_path.is_relative_to(root_dir) else file_path
                out_f.write(f"[{i}] 文件: {rel_path} : 第 {start_line} 行\n")
                out_f.write("-" * 60 + "\n")
                for line in comment_lines:
                    out_f.write(f"{line}\n")
                out_f.write("\n" + "="*80 + "\n\n")
        
        print(f"\n✅ 提取完成！共找到 {len(final_comments)} 个符合条件的长注释块。")
        print(f"结果已保存至: {args.output}")
        
    except Exception as e:
        print(f"❌ 写入输出文件时出错: {e}")

if __name__ == "__main__":
    main()
